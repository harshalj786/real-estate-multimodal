## ðŸ  Multimodal Real Estate Price Prediction

Predict residential property prices by fusing structured housing data with Sentinel-2 satellite imagery. The project benchmarks tabular-only, image-only and multimodal regressors, then uses Grad-CAM to surface neighborhood context that drives model confidence.

---

## ðŸ“Œ Overview

Traditional real estate models thrive on tabular features (square footage, bedrooms, location), yet they overlook environmental signals such as nearby water, green density or urban sprawl. This project asks a simple question: *does high-level satellite context improve price estimation, or at least help explain predictions?*

Whatâ€™s inside:

- âœ… Strong tabular baseline with feature engineering
- ðŸ›°ï¸ ResNet-based visual encoder for satellite tiles
- ðŸ”— Late-fusion multimodal regressor
- ðŸ” Grad-CAM overlays for spatial interpretability

Accuracy gains remain honestâ€”imagery mainly enhances storytelling rather than raw metrics.

---

## ðŸ“‚ Dataset

### Tabular
- **Source:** King County Housing Dataset
- **Target:** Sale price (trained on the log-transformed target)
- **Core features:** bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, latitude/longitude, neighborhood averages (sqft_living15, sqft_lot15)
- **Engineered signals:** house_age, basement_ratio, living_lot_ratio, living_vs_neighbors, is_renovated

### Imagery
- **Source:** Sentinel-2 tiles fetched by latitude/longitude
- **Resolution:** ~10 m/pixel (neighborhood scale)
- **Captures:** tree cover, shoreline proximity, urban densityâ€”not house facades

---

## ðŸ§  Models & Results

| Model | Architecture | RMSE | Notes |
| :--- | :--- | :--- | :--- |
| **Tabular-only** | MLP (Engineered Features) | **~0.31** | ðŸ† **Best Model**. Captures precise property details. |
| **Multimodal** | ResNet-18 + MLP | ~0.45 | Adds visual context but introduces noise. |
| **Image-only** | ResNet-18 (Visual) | ~1.32 | Satellite resolution (10m) is too coarse for pricing. |

> **Insight:** Tabular features drive accuracy. Imagery is best used for **explainability** (identifying water/density) rather than improving raw error metrics.

---

## ðŸ” Explainability

Grad-CAM heatmaps are generated from the multimodal modelâ€™s convolutional backbone. Observed behavior:

- Highlights shorelines, rivers and other water bodies
- Responds to dense urban grids versus suburban sprawl
- Spreads attention across neighborhoods rather than specific homes

Artifacts are exported to `outputs/gradcam/` for inspection.

---

## ðŸ—‚ï¸ Repository Layout

```
real-estate-multimodal/
â”œâ”€ data/
â”‚  â”œâ”€ raw/
â”‚  â”œâ”€ images/
â”‚  â”œâ”€ train_processed.csv
â”‚  â””â”€ test_processed.csv
â”œâ”€ notebooks/
â”‚  â”œâ”€ preprocessing.ipynb
â”‚  â””â”€ model_training.ipynb
â”œâ”€ outputs/
â”‚  â”œâ”€ predictions.csv
â”‚  â””â”€ gradcam/
â”œâ”€ src/
â”‚  â”œâ”€ data_fetcher.py
â”‚  â”œâ”€ dataset.py
â”‚  â””â”€ model.py
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ report.pdf
```

---

## âš™ï¸ Quickstart

1. **Install dependencies**
	```bash
	pip install -r requirements.txt
	```
2. **Preprocess data** â€” run `notebooks/preprocessing.ipynb`
3. **Train + Grad-CAM** â€” run `notebooks/model_training.ipynb`
4. **Predictions** land in `outputs/predictions.csv`

> Tip: ensure Sentinel Hub credentials are configured in `.env` before fetching imagery.

---

## ðŸ§¾ Notes

- Tabular-only model is production-ready due to the lowest RMSE.
- Multimodal variant doubles as an interpretability tool.
- Results emphasise transparency over aggressive leaderboard chasing.
- Every notebook is parameterised to run from the project root for reproducibility.